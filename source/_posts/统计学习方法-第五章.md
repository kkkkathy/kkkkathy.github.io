---
title: 统计学习方法-第五章
date: 2018-12-10 15:14:57
tag: [读书笔记,ML]
categories: 统计学习方法
---

> topic：决策树
>
> author：Kathy Lau

# 决策树

## 决策树模型与学习

- 决策树由节点（node）和有向边（directed edge）组成，结点分为内部结点（internal node）和叶节点（leaf node）。内部结点表示一个特征或属性，叶结点表示一个类。
- 决策树是一个if-then规则的集合，决策树的路径具有一个重要的性质：互斥并且完备（即每一个实例都被且只被一条路径所覆盖）
- 决策树学习本质是从训练数据集中归纳出一组分类规则，是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。
- 决策树学习用损失函数来实现目标，通常是使用正则化的极大似然函数。

### 决策树学习

- 从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习通常采用**启发式方法**近似求解，这样的决策树是此最优（sub-optimal）的。

----

**NP(Non-deterministic Polynomial)——多项式复杂程度的非确定性问题。**

- 在设计程序时，我们经常需要评估这个程序的时间复杂度，即衡量当问题规模变大后，程序执行所需的时间增长会有多快。
  - O(1)表示常数级别，即不管问题的规模变大多少倍，所耗的时间不会改变
  - $O(N^2)$表示平方级别，即当问题规模增大至2倍时，所花费的时间则放大至4倍
  - $O(2^n)$表示指数级别，即当问题规模倍数扩大时，所用时间会呈指数放大
- **多项式时间**则是指$O(1),O(logN),O(N^2)$等这类可用多项式表示的时间复杂度，通常我们认为计算机可解决的问题只限于多项式时间内。而$O(2^n),O(N!)$这类非多项式级别的问题，其复杂度往往已经到了计算机都接受不了的程度。
- 所有非确定性多项式时间内可解的判定问题构成**NP类问题**
  - NP类问题将问题分为求解和验证两个阶段，问题的求解是非确定性的，无法在多项式时间内得到答案，而问题的验证却是确定的，能够在多项式时间里确定结果。
- NP中的一类比较特殊的问题，这类问题中每个问题的复杂度与整个类的复杂度有关联性，假如其中任意一个问题在多项式时间内可解的，则这一类问题都是多项式时间可解。这些问题被称为**NP完全问题**。


----

---

**启发式算法**

一个基于直观或经验构造的算法，在可接受的花费（指计算时间和空间）下给出待解决组合优化问题每一个实例的一个可行解，该可行解与最优解的偏离程度一般不能被预计。

启发式算法的难点是建立符合实际问题的一系列启发式规则。

启发式算法的优点在于它比盲目型的搜索法要高效，一个经过仔细设计的启发函数，往往在很快的时间内就可得到一个搜索问题的最优解，对于NP问题，亦可在多项式时间内得到一个较优解。

----

**决策树学习过程：**

- 构建根节点，从训练数据中选择一个最优特征，将训练数据分割成子集，使得自己在当前条件下有一个最好的分类。
- 如果子集能基本被正确分类，构建叶节点
- 如果还有子集不能被正确分类，就对子集选择新的最优特征，继续分割构建相应节点。
- 直到所有训练数据子集都被基本正确分类或者没有合适的特征。



以上学习过程很可能发生过拟合现象，因此需要对已生成的树自下而上进行剪枝，将树变得更简单，有更好的泛化能力：

- 去掉过分细分的叶节点，使其回到父节点或者更高的结点，然后将对应的父节点改为新的叶节点


- 如果特征数量很多，也可以在决策树学习开始时，对特征进行选择，即留下对训练数据有足够分类能力的特征
- 决策树的生成对应于模型的局部最优选择；决策树的剪枝操作考虑全局最优选择
- 决策树学习常用算法有ID3，C4.5，CART




## 特征选择

特征选择的准则是信息增益（information gain）或信息增益比：信息增益表示得知特征X的信息而使得类Y的信息不确定性减少的程度。

---

### 熵

- 表示随机变量不确定性的度量

- 若X是一个取有限个值的离散随机变量，其概率分布为：
  $$
  P(X=x_i)=p_i,\ \ \ \ i=1,2,...,n
  $$






- 则随机变量X的熵定义为：
  $$
  H(X)=-\sum_{i=1}^np_ilog\ p_i
  $$
  其中定义$0log0=0$，由定义可得，熵只依赖于X的分布，与X取值无关，即X的熵可以计作$H(p)$。

- 熵越大，随机变量的不确定性就越大。从定义可验证：
  $$
  0\leq H(p)\leq log\ n
  $$






### 条件熵

- 舍友随机变量$(X,Y)$，其联合概率分布为：
  $$
  P(X=x_i,Y=y_i)=p_{ij},\ \ \ \ i=1,2,...,n;\ \ j=1,2,...,m
  $$






- 条件熵$H(Y|X)$表示已知随机变量X时随机变量Y的不确定性，定义为X给定条件下Y的条件概率分布熵对X的数学期望：
  $$
  H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)
  $$
  这里$p_i=P(X=x_i),\ \ \ i=1,2,...,n$。

---

### 信息增益

- 特征A对训练数据集D的信息增益$g(D, A)$，定义为集合D的经验熵$H(D)$与给定特征A条件下D的经验条件熵$H(D|A)$之差：

$$
g(D, A)=H(D)-H(D|A)
$$

​	一般的，这种差值称之为互信息(mutual information)，信息增益大的特征具有更强的分类能力

- 信息增益的算法： 

  ---

  训练数据集为D，|D|表示样本总量
  样本设有K个类$C_k$，$|C_k|$表示属于类$C_k$的样本个数

  特征A的取值将D划分为n个子集，$|D_i|$表示属于这个子集的样本个数

  子集$D_i$属于类$C_k$的样本集合计作$D_{ik}$。

  ---

  - 输入：训练数据集D和特征A；

  - 输出：特征A对训练数据集D的信息增益$g(D,A)$

    a. 计算数据集D的经验熵$H(D)$
    $$
    H(D)=-\sum_{k=1}^K{\frac{|C_k|}{|D|}log_2{\frac{|C_k|}{|D|}}}
    $$




​		b. 计算特征A对数据集D的经验条件熵$H(D|A)$
$$
H(D|A)=\sum_{i=1}^n\frac{|D_i|}{D}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|Di|}log_2\frac{|D_{ik}|}{|D_i|}
$$
​		c. 计算信息增益
$$
g(D,A)=H(D)-H(D|A)
$$


### 信息增益比（information gain ratio）

- 使用信息增益作为划分，存在**偏向于选择取值较多的特征**的问题，使用信息增益比可以进行对应的校正。

- 特征A对训练数据集D的信息增益比$g_{R}(D,A)$定义为：信息增益$g(D,A)$与训练数据D关于特征A的值的熵$H_A(D)$之比，即：
  $$
  g_R(D,A)=\frac{g(D,A)}{H_A(D)}
  $$
  其中，$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{D}log_2\frac{D_i}{D}$，n是特征A取值的个数。




## 决策树的生成

### ID3算法

算法核心：在决策树各个结点应用信息增益准则选择特征，递归构建决策树。

- 输入：训练数据集D，特征集A，阈值$\varepsilon$

- 输出：决策树T

  a. 若D中所有实例属于同一类$C_k$，则T为单结点树，将类$C_k$作为该结点的类标记，返回T；

  b. 若$A=\oslash$，则T为单结点树，将D中实例数最大的类$C_k$作为该结点的类标记，返回T；

  c. 否则，按<u>信息增益算法</u>计算各特征对D的信息增益，选择信息增益最大的特征$A_g$

  d. 如果$A_g$的信息增益小于阈值$\varepsilon$，则置T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T；

  e. 否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将D分割为若干个非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T；

  f. 对第i个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归调用步骤a到步骤e，得到子树$T_i$，返回$T_i$

该算法只有树的生成，容易产生过拟合。



### C4.5生成算法

c4.5对ID3算法做了改进，即**使用了信息增益比来选择特征**。

- 输入：训练数据集D，特征集A，阈值$\varepsilon$

- 输出：决策树T

  a. 若D中所有实例属于同一类$C_k$，则T为单结点树，将类$C_k$作为该结点的类标记，返回T；

  b. 若$A=\oslash$，则T为单结点树，将D中实例数最大的类$C_k$作为该结点的类标记，返回T；

  c. 否则，按<u>信息增益比算法</u>计算各特征对D的信息增益，选择信息增益比最大的特征$A_g$

  d. 如果$A_g$的信息增益比小于阈值$\varepsilon$，则置T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T；

  e. 否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将D分割为若干个非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T；

  f. 对第i个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归调用步骤a到步骤e，得到子树$T_i$，返回$T_i$




## 决策树的剪枝

为了防止在决策树生成过程中出现过拟合现象，构造出过于复杂的决策树，我们考虑决策树的复杂度，对已生成的决策树进行简化——剪枝（pruning）。

- 决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。

- 设树的叶结点个数为$|T|$，t是树T的叶结点，叶结点上有$N_t$个样本点，其中k类样本点有$N_{tk}$个，$H_t(T)$为叶结点t上的经验熵，$\alpha$为参数，则决策树学习的损失函数定义如下：
  $$
  C_\alpha(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|
  $$

  $$
  H_t(T)=-\sum_k\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}
  $$







- 我们令右边第一项为C(T)，即：
  $$
  C_\alpha(T)=C(T)+\alpha|T|
  $$
  其中，C(T)表示模型对训练数据的预测误差，即与训练数据的拟合程度，|T|表示模型复杂度，参数$\alpha\geq0​$控制两者之间的影响。

  - 较大的$\alpha$促使选择较简单的模型，较小的$\alpha$促使选择较复杂的模型，$\alpha=0$意味着不考虑模型复杂度。

- 剪枝是当$\alpha$确定时，选择损失函数最小的模型，即子树。当$\alpha$确定时，子树越大，拟合效果越好，复杂度越高，损失函数表示了对两者的平衡。

- 通过剪枝可以减小模型的复杂度。

**树的修剪算法如下：**

- 输入：生成算法的整个树T，参数$\alpha$

- 输出：修剪后的子树$T_\alpha$

  a. 计算每个结点的经验熵

  b. 递归从树的叶结点向上回缩：回缩到父节点之前与之后的整体树分别为$T_B$和$T_A$，如果$C_\alpha(T_A)\leq C_\alpha(T_B)$，则进行剪枝，将父节点变为新的叶结点

  c. 返回b，直到不能继续为止，得到损失函数最小的子树$T_\alpha$




## CART算法

classification and regression tree，分类与回归树，是应用广泛的决策树学习方法。

CART算法是在给定输入随机变量X的条件下输出随机变量Y的条件概率分布的学习方法，由以下两步组成：

- 决策树生成：基于训练数据生成，决策树要尽量大
- 决策树剪枝：根据损失函数最小选择最优子树

决策树的生成时递归的构建二叉决策树的过程：

- 对回归树用平方误差最小化准则进行特征选择
- 对分类树使用基尼(gini)准则最小化准则进行特征选择



### 回归树的生成

使用最小二乘回归树生成算法，具体如下：

- 输入：训练数据集D

- 输出：回归树$f(x)$

  **在训练数据集输入空间中，递归将每个区域划分为两个子区域并决定每个子区域的输出值，构建二叉决策树。**

  **假设已将输入空间划分为M个单位$R_1, R_2,...,R_M$，在每个单位$R_m$上有一个固定的输出值$c_m$，最优值$\hat c_m$是$R_m$上所有实例对应输出的均值。**

  a. 我们令第j个变量的取值为s，选择最优切分变量j与切分点s，求解：


$$
min_{j,s}[min_{c1}\sum_{x_i\in R_1(j,s)}(y_i-c_i)^2+min_{c2}\sum_{x_i\in R_2(j,s)}(y_i-c_i)^2]
$$
​	遍历变量j，对固定的切分变量j扫描切分点s，找到使得式子最小的$(j,s)$对。

​	b. 用选定的$(j,s)$对划分区域并决定相应的输出值：
$$
R_1(j,s)=\{x|x^{(j)}\leq s\},\ \ \ R_2(j,s)=\{x|x^{(j)}> s\}
$$

$$
\hat c_m=\frac{1}{N_m}\sum_{x_i\in R_m(j,s)}y_i,\ \ x\in R_m,\ \ m=1,2
$$

​	c. 继续对两个子区域调用步骤a，b，直到满足停止条件。

​	d. 将输入空间划分为M个区域$R_1,R_2,...,R_M​$，生成决策树：
$$
f(x)=\sum_{m=1}^M\hat c_mI(x\in R_m)
$$



### 分类树的生成

分类树使用基尼指数选择最优特征，同时决定该特征的最优二值切分点。

#### 基尼指数

a. 在分类问题中，假设有K个类，样本点属于第K类的概率为$p_k$，则概率分布的尼基指数定义为：
$$
Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2
$$
b. 对于给定的样本集合D，其基尼指数为：
$$
Gini(D)=1-\sum_{k=1}^K(\frac{|C_k|}{D})^2
$$
这里$C_k$是D中属于第k类的样本子集，K是类的个数。

c. 如果样本集合D根据特征A是否取某个可能值a被分割成$D_1$和$D_2$两部分：
$$
D_1={(x,y)\in D|A(x)=a},\ \ D2=D-D_!
$$
则在特征A的条件下，集合D的基尼指数定义为：
$$
Gini(D,A)=\frac{|D1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D2)
$$
$Gini(D)$表示集合D的不确定性，$Gini(D,A)$表示经$A=a$分割后集合D的不确定性。

基尼指数越大，样本集合的不确定性越大。

#### CART生成算法

- 输入：训练数据集D，停止计算的条件

- 输出：CART决策树

  从根节点开始，递归对每个结点进行操作，构建二叉决策树：

  a. 设结点训练数据集为D，计算现有特征对该数据集的基尼指数：即对每个特征A，其可能取的每个值a，根据样本点测试$A=a$为“是”或“否”将D分割为$D_1$和$D_2$，计算条件基尼指数。

  b.在所有特征A和所有可能的切分点a中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点，从现结点生成两个子结点，将训练数据集特征分配到子结点去。

  c.对两个子结点递归调用a,b，直到满足停止条件

  d.生成CART决策树

  **算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值（样本基本属于同一类），或者没有更多特征**



### CART剪枝

step1：剪枝，形成子树序列

step2：在剪枝得到的子树序列$T_0,T_1,...,T_n$中通过交叉验证选取最优子树$T_\alpha$

#### CART剪枝算法

- 输入：CART算法生成的决策树$T_0$

- 输出：最优决策树$T_\alpha$

  a.设$k=0,T=T_0$

  b.设$\alpha=+\infty$

  c.自下而上对各内部结点t计算$C(T_t)$，$|T_t|$以及
  $$
  g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}
  $$

  $$
  \alpha=min(\alpha,g(t))
  $$





​	这里$T_t$表示以t为根节点的子树，$C(T_t)$是对训练数据的预测误差（如基尼指数），$|T_t|$是$T_t$的叶节点个数

​	d.对$g(t)=\alpha$的内部结点t进行剪枝，并对叶结点t以多数表决法决定其类，得到树T

​	e.设$k=k+1,\alpha_k=\alpha,T_k=T$

​	f.如果$T_k$不是由根节点及两个叶结点构成的树，则回到步骤c，否则令$T_k=T_n$

​	g.采用交叉验证法在子树序列$T_0,T_1,...,T_n$中选取最优子树$T_\alpha$