---
title: 统计学习方法-第一章
date: 2018-12-06 19:24:25
tag: 读书笔记
categories: 统计学习方法
---

> topic：统计学习方法概论
>
> author：刘子璐

### 1.1 统计学习概述

- 统计学习也称统计机器学习（statistical machine learning），即运用数据以及统计方法提高系统性能的机器学习。

- 统计学习的总目标即考虑学习什么模型以及如何学习，在提高准确性的同时提高学习效率。

- 主要包括监督学习、非监督学习、半监督学习、强化学习等类型

- 统计学习方法三要素：方法 = 模型（model）+ 策略（strategy）+ 算法（algorithm）
  - 确定模型选择的准则，即学习策略
  - 实现求解最优模型的算法，即学习算法
  - 通过学习方法选择最优模型

- 计算机科学由三维组成：系统、计算、信息。统计学习属于信息这一维。

  ​



### 1.2 监督学习

- 监督学习中包括输入空间与输出空间，每个具体的输入是一个实例（instance），通常由特征向量来表示（feature vector）。所有特征向量存在的空间为特征空间，每一维对应一个特征。

- 根据预测任务分类：
  - 回归问题：输入变量与输出变量均为连续变量
  - 分类问题：输出变量为有限个离散变量
  - 标注问题：输入与输出变量均为变量序列的预测问题

- 监督学习假设输入与输出的随机变量X与Y遵循联合概率分布$P(X, Y)$，即分布密度函数

- 监督学习的模型可以是条件概率模型$P(Y|X)$或非概率模型（决策函数）$Y=f(X)$

  ​



### 1.3 统计学习三要素

#### 1.3.1 模型

模型的假设空间包含所有可能的条件概率分布或决策函数。可以定义为：


\digamma=\{f|Y=f_\theta(X),\theta\in{R^n}\}

$$
\digamma=\{P|P_\theta(Y|X),\theta\in{R^n}\}
$$


#### 1.3.2 策略

- 使用loss function（损失函数）或者cost function（代价函数）来度量预测错误的程度。

- 损失函数是非负实值函数，记为$L(Y, f(X))$

- 统计学习中常用的损失函数：

  - 0-1损失函数（0-1 loss function）：
    $$
    L(Y,f(X)) = 
    \begin{cases}
    1,  & \text{$Y\neq{f(X)}$} \\[2ex]
    0, & \text{$Y=f(X)$}
    \end{cases}
    $$
    ​

  - 平方损失函数（quadratic loss function）：
    $$
    L(Y,f(X)) = (Y-f(X))^2
    $$
    ​

  - 绝对损失函数(absolute loss function)
    $$
    L(Y,f(X)) = |Y-f(X)|
    $$
    ​

    > - 通常来说，平方损失函数在误差较大点时的损失远大于绝对损失函数，他会赋予异常值更大的权重，模型会去减小异常值造成的误差，使得模型的整体表现下降。所以当训练数据中有较多的异常值时，绝对损失函数更有效。
    >
    >
    > - 但是绝对损失函数存在的问题是，在神经网络中，极值点时梯度会有很大的跃变，所以需要在极值点靠近过程中动态减小学习率。而平方损失函数则在固定学习率下也能收敛。
    >
    >
    > - 总结：绝对损失对于异常值更鲁棒，但它导数不连续使得寻找最优解的过程低效；平方损失对于异常值敏感，但在优化过程中更为稳定和准确。

    ​

  - 对数损失函数(logarithmic loss function)or对数似然损失函数(log-likelihood loss function):

  - $$
    L(Y,P(Y|X)) = -logP(Y|X)
    $$






- 损失函数越小，模型越好。损失函数的期望函数（risk function）或称期望损失（expected loss）如下，也称为泛化误差：
  $$
  R_{exp}(f)=E_p[L(Y, f(X))]=\int_{x*y}L(y, f(x))P(x, y)dxdy
  $$






- 在实际的处理中，联合分布P是未知的，所以监督学习的损失函数期望是个未知函数。这时很自然的想到使用经验风险来估计期望风险，即：
  $$
  R_{exp}(f)=\frac{1}{N}\sum_{i=1}^NL(Y, f(X))
  $$

  > **根据大数定律，当样本容量N趋于无穷。经验风险会趋于期望风险。而实际中样本数目有限，所以需要使用两种基本策略：经验风险最小化和结构风险最小化。**

  ​

##### 经验风险最小化

- ERM：认为经验风险最小的模型就是最优模型。
- 极大似然估计（maximum likelihood estimation）就是经验风险最小化的一个例子
- 样本容量小时，这个效果容易出现过拟合（over-fitting）现象。

##### 结构风险最小化

- SRM：防止过拟合而提出的策略，等价于**正则化**（regularization），即在经验风险上加上表示模型复杂度的正则化项（regularizer）或惩罚项（penalty term），定义为：
  $$
  R_{srm}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda{J(f)}
  $$

  - $J(f)$为模型的复杂度，是定义在假设空间上的泛函数。
  - 模型越复杂，J就会越大，反之越小，即复杂度表示对复杂模型的惩罚。
  - $\lambda\geq0$是系数，用于权衡经验风险和模型复杂度。
  - 结构风险小需要经验风险与模型复杂度同时小。
  - 贝叶斯估计中最大后验概率估计（MAP）就是结构风险最小化的例子。

> 从而，监督学习问题就变成了经验风险或结构风险函数的最优化问题。经验或结构风险函数即成为优化的目标函数。



### 1.4 模型评估与模型选择

- 训练误差的大小对于判断问题是不是一个容易学习的问题有意义，而测试误差反映了学习方法对未知的测试数据集的预测能力。
- 学习方法对未知数据的预测能力称为泛化能力（generalization ability）
- 过拟合（over-fitting）：学习时选择的模型包含参数过多，以至于出现对已知数据预测很好，对未知数据预测很差的现象
- 当模型的复杂度增大时，训练误差会逐渐减小并趋向于0；测试误差会先减小后增大。最优的模型选择即选择复杂度适当的模型，已达到测试误差最小的学习目的。两种常见的模型选择方法为：**正则化与交叉验证**

##### 正则化（regularization）

- 结构风险最小化的实现，正则化项一般是模型复杂度的单调递增函数（eg：模型参数向量的范数）
- 范数：在数学上，范数包括向量范数和矩阵范数，向量范数表征向量空间中向量的大小，矩阵范数表征矩阵引起变化的大小：
  - L0范数：向量$x$中非零元素的个数   $||x||_0 = \#(i|x_i\neq0)$
  - L1范数：向量$x$中非零元素的绝对值之和   $||x||_1 = \sum_i|x_i|$
  - L2范数：向量元素的平方和再开平方    $||x||_2 = \sqrt{\sum_ix_i^2}$    （欧氏距离）

##### 交叉验证（cross validation）

- 当样本数据充足时，可以将数据集切分为三部分，分别为训练集（训练模型），验证集（模型的选择）和测试集（最终的方法评估），最终选择对验证机有最小预测误差的模型。
- 当数据不充足时，可以采用交叉验证的方法：重复使用数据，对数据进行切分，组合为训练集与测试集，在此基础上反复训练、测试以及模型选择：
  - 简单交叉验证：随机将数据分为两部分，一部分为训练集，一部分为测试集。使用训练集在各种条件下训练模型，在测试集评价测试误差，选出最好的模型。
  - S折交叉验证：随机将数据切分为S个互不相交的子集，利用S-1个子集训练模型，利用余下的子集测试模型。将这一过程对可能的S种重复进行，选择S此评测平均测试误差最小的模型。
  - 留一交叉验证：S折交叉验证的特殊情形，即$S=N$，往往在数据缺乏的情况下使用。



### 1.5 生成模型与判别模型

- 生成方法：由数据学习联合概率分布，然后求出条件概率分布$P(Y|X)$作为预测的模型。
  - 典型模型有：朴素贝叶斯法、隐马尔科夫模型
  - 特点：可还原联合概率分布；学习收敛速度更快；存在隐变量时，仍可以学习
- 判别方法由数据直接学习决策函数或者条件概率分布。
  - 典型模型有：k近邻法、感知机、决策树、逻辑回归、最大熵模型、支持向量机、提升方法、条件随机场等
  - 学习准确率往往更高；可以对数据进行抽象、定义与使用特征，可以简化学习问题。



### 1.6 预测任务类别

#### 1.6.1 分类问题

- 评价分类器性能的指标一般是分类准确率（accuracy：对于给定的测试数据集，分类器正确分类的样本数与总样本数之比）

- 二分类常用为精确率（precision）与召回率（recall）

  - TP（True Positive）——将正类预测为正类数

  - FN（False Negative）——将正类预测为负类数

  - FP（False Positive）——将负类预测为正类数

  - TN（True Negative）——将负类预测为负类数

  - 精确率：
    $$
    P=\frac{TP}{TP+FP}
    $$
    ​

  - 召回率：
    $$
    P=\frac{TP}{TP+FN}
    $$





- F1(精确率和召回率的调和均值)，精确率和召回率都高时，F1值也会高：
  $$
    \frac{2}{F_1}=\frac{1}{P} + \frac{1}{R}
  $$





#### 1.6.2 标注问题

- 标注问题是分类问题的一种推广
- 常用方法：隐马尔科夫模型，条件随机场



#### 1.6.3 回归问题

- 回归问题等价于函数拟合
- 分类：
  - 按照输入变量的个数：一元回归、多元回归
  - 按照模型类型：线性回归、非线性回归
- 回归学习最常用的损失函数为**平方损失函数**，此情况下可以由最小二乘法求解