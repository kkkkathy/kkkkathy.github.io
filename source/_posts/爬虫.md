---
title: 爬虫
date: 2018-12-06 18:40:43
tags:
---

# 爬虫学习与问题总结

## 遇到的问题：

### 解析网页

beautiful soap默认使用lxml进行解析，因为它的解析速度快，并且容错能力强。而在我进行数据爬取的过程中，出现了源码丢失的问题，导致想要爬到的数据缺失。通过查找博客与相关文档，提出了可能出现的两个问题：

- BeautifulSoup 有时候会遇到非法的，不支持的 html 源码而导致无法解析或无法正常解析 html；
- 处理的文档太大，而处理的解析器缓存不够造成的信息丢失。

**解决方法**：将“lxml”换成了“html.parser”



### 数据库存储过慢

要将爬取的数据放在数据库中，我的初始想法是，在遍历多个页面时，每次存储一个页面要获取的所有数据，然后将其插入到数据库中，即存入数据库的函数每次获得的参数是一个页面所有数据（列表或多个列表）。但是在运行时发现插入速度非常慢，当爬取页面较多且数据量较大时，太耗时了。

**解决方法**：

- 采取多线程进行优化
- 存入数据库传入的参数，包含了多个页面的数据及文本内容，即多次爬取，一次插入